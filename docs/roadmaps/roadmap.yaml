roadmap_version: '1.0'
github_enabled: true
github_repo: paiml/realizar
roadmap:
- id: PARITY-001
  github_issue: null
  item_type: task
  title: 'KV Cache Integration: trueno-db MemoryKvStore into GGUFTransformer'
  status: inprogress
  priority: critical
  assigned_to: null
  created: 2025-12-13T16:30:00+00:00
  updated: 2025-12-13T16:35:59.510588426+00:00
  spec: docs/specifications/performance-parity-ollama-llamacpp-gpu-inference-llms.md
  acceptance_criteria:
  - 'FALSIFIABLE: 1,090x gap reduces to <15x after KV cache integration'
  - 'METRIC: tok/s increases from 0.22 to >18 tok/s'
  - 'TEST: cargo test --lib test_kv_cache_integration passes'
  - 'VERIFY: cargo run --example imp_800_kv_cache_falsification confirms speedup'
  - Integrate trueno-db v0.3 MemoryKvStore into attention mechanism
  - Cache K/V tensors per layer per position
  - Skip K/V recomputation for cached positions
  - Unit tests with 85% coverage on KV cache path
  phases: []
  subtasks: []
  estimated_effort: 3 days
  labels:
  - perf-parity
  - kv-cache
  - trueno-db
  - p0-critical
  - IMP-800
  notes: 'VERIFIED via IMP-800: 128x theoretical speedup. Subtasks: 001a-Add trueno-db dependency, 001b-Create KVCacheManager, 001c-Wire into forward(), 001d-Benchmark, 001e-Measure gap'
- id: PARITY-002
  github_issue: null
  item_type: task
  title: 'FlashAttention CUDA: trueno-gpu AttentionKernel for Prompts'
  status: planned
  priority: high
  assigned_to: null
  created: 2025-12-13T16:30:00+00:00
  updated: 2025-12-13T16:30:00+00:00
  spec: docs/specifications/performance-parity-ollama-llamacpp-gpu-inference-llms.md
  acceptance_criteria:
  - 'FALSIFIABLE: Gap reduces from ~8.5x to <6x after FlashAttention'
  - 'METRIC: Prompt processing time reduces by >10x'
  - 'TEST: cargo test --lib test_flash_attention_cuda --features cuda passes'
  - 'VERIFY: cargo run --example imp_801_flash_attention_falsification --features cuda'
  - Integrate trueno-gpu AttentionKernel (PTX generation)
  - Use tiled attention (B_r=64, B_c=64) for O(N) memory
  - Support causal masking for autoregressive models
  - Online softmax (never materialize NxN matrix)
  phases: []
  subtasks: []
  estimated_effort: 4 days
  labels:
  - perf-parity
  - flash-attention
  - trueno-gpu
  - cuda
  - p1-high
  - IMP-801
  notes: 'VERIFIED via IMP-801: 16x conservative speedup. Depends on PARITY-001. Subtasks: 002a-Add cuda feature, 002b-Create CudaAttention, 002c-Integrate, 002d-Benchmark, 002e-Measure'
- id: PARITY-003
  github_issue: null
  item_type: task
  title: 'Q4_K Fused Operations: Dequant+Matvec in Single Pass'
  status: planned
  priority: high
  assigned_to: null
  created: 2025-12-13T16:30:00+00:00
  updated: 2025-12-13T16:30:00+00:00
  spec: docs/specifications/performance-parity-ollama-llamacpp-gpu-inference-llms.md
  acceptance_criteria:
  - 'FALSIFIABLE: Gap reduces from ~5x to <2x after fused Q4_K'
  - 'METRIC: Memory bandwidth utilization >80%'
  - 'TEST: cargo test --lib test_fused_q4k_matvec passes'
  - 'BENCHMARK: cargo bench --bench quantize -- fused_q4k shows >25x vs naive'
  - Implement fused dequantize+matvec kernel (no intermediate F32 buffer)
  - SIMD-optimized for AVX2/AVX-512/NEON
  - Block size 32 aligned with GGUF Q4_K format
  phases: []
  subtasks: []
  estimated_effort: 4 days
  labels:
  - perf-parity
  - quantization
  - q4k
  - simd
  - p1-high
  - IMP-100
  notes: 'VERIFIED via IMP-100c: 29-132x speedup. Subtasks: 003a-Profile memory, 003b-Implement fused, 003c-SIMD specializations, 003d-Integrate, 003e-Benchmark'
- id: PARITY-004
  github_issue: null
  item_type: task
  title: 'Multi-Accumulator SIMD: 4-way Parallel Dot Products'
  status: planned
  priority: medium
  assigned_to: null
  created: 2025-12-13T16:30:00+00:00
  updated: 2025-12-13T16:30:00+00:00
  spec: docs/specifications/performance-parity-ollama-llamacpp-gpu-inference-llms.md
  acceptance_criteria:
  - 'FALSIFIABLE: Dot product throughput increases by >2x'
  - 'METRIC: GFLOPS for dot product >150 on AVX2'
  - 'TEST: cargo test --lib test_multi_accumulator_dot passes'
  - 'VERIFY: Match llama.cpp GGML_F32_ARR pattern (4 accumulators)'
  - Implement 4-accumulator dot product (hide FMA latency)
  - Unroll loop by 4 for instruction-level parallelism
  - Apply to attention score computation and matvec
  phases: []
  subtasks: []
  estimated_effort: 2 days
  labels:
  - perf-parity
  - simd
  - dot-product
  - p2-medium
  - IMP-500
  notes: 'VERIFIED via IMP-500c: 2.3x speedup measured. Low effort, high impact.'
- id: PARITY-005
  github_issue: null
  item_type: task
  title: 'Memory Layout Optimization: Contiguous KV for Cache Efficiency'
  status: planned
  priority: medium
  assigned_to: null
  created: 2025-12-13T16:30:00+00:00
  updated: 2025-12-13T16:30:00+00:00
  spec: docs/specifications/performance-parity-ollama-llamacpp-gpu-inference-llms.md
  acceptance_criteria:
  - 'FALSIFIABLE: L2 cache hit rate increases from <70% to >90%'
  - 'METRIC: Memory stalls reduced by >30%'
  - 'TEST: cargo test --lib test_contiguous_kv_layout passes'
  - Restructure KV cache layout for sequential access
  - Align to 64-byte cache lines
  phases: []
  subtasks: []
  estimated_effort: 2 days
  labels:
  - perf-parity
  - memory
  - cache
  - p2-medium
  notes: Targets memory-bound bottleneck. Critical for CPU inference.
- id: PARITY-006
  github_issue: null
  item_type: task
  title: 'Batch Processing: Parallel Token Generation'
  status: planned
  priority: medium
  assigned_to: null
  created: 2025-12-13T16:30:00+00:00
  updated: 2025-12-13T16:30:00+00:00
  spec: docs/specifications/performance-parity-ollama-llamacpp-gpu-inference-llms.md
  acceptance_criteria:
  - 'FALSIFIABLE: Batch throughput >2x single-request throughput'
  - 'METRIC: GPU utilization >60% under batch load'
  - 'TEST: cargo test --lib test_batch_generation passes'
  - Implement batched forward pass for multiple requests
  - Share KV cache across batch where applicable
  - Use GPU GEMM for batch matmul (IMP-600c verified 57x)
  phases: []
  subtasks: []
  estimated_effort: 3 days
  labels:
  - perf-parity
  - batch
  - gpu
  - p2-medium
  - IMP-600
  notes: 'VERIFIED via IMP-600c: GPU 57x faster for GEMM. Use GPU for batch, SIMD for single.'
- id: PARITY-007
  github_issue: null
  item_type: task
  title: 'Parity Verification: E2E Benchmark vs Ollama'
  status: planned
  priority: low
  assigned_to: null
  created: 2025-12-13T16:30:00+00:00
  updated: 2025-12-13T16:30:00+00:00
  spec: docs/specifications/performance-parity-ollama-llamacpp-gpu-inference-llms.md
  acceptance_criteria:
  - 'FALSIFIABLE: Gap to Ollama <1.25x on identical hardware'
  - 'METRIC: Realizar tok/s within 80% of Ollama tok/s'
  - 'REPRODUCIBLE: CV < 0.05 across 10 runs'
  - 'TEST: make bench-server-matrix shows parity'
  - Run standardized benchmark suite against Ollama
  - Document hardware configuration (GPU, RAM, CPU)
  - Report p50, p95, p99 latencies
  - Calculate and report CV for statistical validity
  phases: []
  subtasks: []
  estimated_effort: 1 day
  labels:
  - perf-parity
  - benchmark
  - verification
  - p3-low
  - IMP-700
  notes: Final verification step. Run after PARITY-001 through PARITY-006 complete.
- id: PARITY-008
  github_issue: null
  item_type: task
  title: 'Popper Score Improvement: Add Measurable Thresholds'
  status: planned
  priority: low
  assigned_to: null
  created: 2025-12-13T16:30:00+00:00
  updated: 2025-12-13T16:30:00+00:00
  spec: docs/specifications/performance-parity-ollama-llamacpp-gpu-inference-llms.md
  acceptance_criteria:
  - 'FALSIFIABLE: Popper score increases from 79 to >90'
  - 'METRIC: Category A (Falsifiability) increases from 72% to >90%'
  - A1 (Measurable Thresholds) increases from 2/8 to >6/8
  - Add explicit falsifiable claims with numeric thresholds
  - Document all metrics with success/failure criteria
  - Add random seed management for reproducibility
  phases: []
  subtasks: []
  estimated_effort: 1 day
  labels:
  - perf-parity
  - popper
  - quality
  - p3-low
  notes: 'Current Popper score: 79/100 (B). Target: 90+ (A).'
- id: IMP-700
  github_issue: null
  item_type: task
  title: 'COMPLETED: Real-World Verification vs Ollama'
  status: completed
  priority: critical
  assigned_to: null
  created: 2025-12-13T00:00:00+00:00
  updated: 2025-12-13T16:00:00+00:00
  spec: docs/specifications/performance-parity-ollama-llamacpp-gpu-inference-llms.md
  acceptance_criteria:
  - Measure Ollama throughput (verified 240.1 tok/s)
  - Measure Realizar throughput (verified 0.22 tok/s)
  - Calculate gap (verified 1,090x)
  - Low CV indicates stable measurements (CV=0.0388)
  phases: []
  subtasks: []
  estimated_effort: null
  labels:
  - perf-parity
  - verification
  - completed
  notes: 'Established baseline gap: 1,090x. Foundation for all optimization work.'
- id: IMP-800
  github_issue: null
  item_type: task
  title: 'COMPLETED: KV Cache Falsification'
  status: completed
  priority: critical
  assigned_to: null
  created: 2025-12-13T00:00:00+00:00
  updated: 2025-12-13T16:00:00+00:00
  spec: docs/specifications/performance-parity-ollama-llamacpp-gpu-inference-llms.md
  acceptance_criteria:
  - Verify KV cache provides 10-100x speedup (verified 128x average)
  - Range 4.5x to 512x depending on sequence length
  - trueno-db MemoryKvStore capabilities confirmed
  phases: []
  subtasks: []
  estimated_effort: null
  labels:
  - perf-parity
  - kv-cache
  - falsification
  - completed
  notes: 'VERIFIED: 128x speedup theoretical. Next step: integration (PARITY-001).'
- id: IMP-801
  github_issue: null
  item_type: task
  title: 'COMPLETED: FlashAttention CUDA Falsification'
  status: completed
  priority: high
  assigned_to: null
  created: 2025-12-13T00:00:00+00:00
  updated: 2025-12-13T16:00:00+00:00
  spec: docs/specifications/performance-parity-ollama-llamacpp-gpu-inference-llms.md
  acceptance_criteria:
  - Verify FlashAttention provides 10-50x speedup (verified 16x conservative)
  - Scales with sequence length (2x at 128, 32x at 2048)
  - trueno-gpu AttentionKernel capabilities confirmed
  phases: []
  subtasks: []
  estimated_effort: null
  labels:
  - perf-parity
  - flash-attention
  - falsification
  - completed
  notes: 'VERIFIED: 16x speedup conservative. Next step: integration (PARITY-002).'
- id: IMP-600
  github_issue: null
  item_type: task
  title: 'COMPLETED: GPU Capability Falsification'
  status: completed
  priority: high
  assigned_to: null
  created: 2025-12-13T00:00:00+00:00
  updated: 2025-12-13T16:00:00+00:00
  spec: docs/specifications/performance-parity-ollama-llamacpp-gpu-inference-llms.md
  acceptance_criteria:
  - GPU vs SIMD for MATVEC (FALSIFIED - GPU 2.7x SLOWER)
  - GPU vs SIMD for GEMM (VERIFIED - GPU 57x FASTER)
  - Conclusion - Use SIMD for token gen, GPU for batch/prompt
  phases: []
  subtasks: []
  estimated_effort: null
  labels:
  - perf-parity
  - gpu
  - falsification
  - completed
  notes: 'CRITICAL INSIGHT: GPU hurts single-token, helps batch. Informed PARITY-006 design.'
