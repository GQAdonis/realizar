//! EXTREME TDD: Scheduler Coverage Tests
//!
//! Per spec: Tests for batch scheduling edge cases, request prioritization,
//! queue management, and error handling paths in src/scheduler.rs.
//!
//! Target: Increase coverage from 90% to 98%+

use realizar::paged_kv::PagedKvCache;
use realizar::scheduler::{
    BatchConfig, BatchScheduler, BatchStats, BatchToken, BatchType, ChunkedPrefillConfig,
    ChunkedPrefillScheduler, ChunkedPrefillState, ChunkedPrefillStats, Deadline,
    DynamicPriorityConfig, DynamicPriorityScheduler, DynamicRequest, DynamicSchedulerStats,
    MicroBatch, Priority, Scheduler, SchedulerError, SchedulerOutput, SchedulerRequest,
    SchedulerStats, SequenceBatch, SequenceBatchEntry, SequenceState, Slot, SlotManager,
    SlotState,
};
use std::collections::HashMap;
use std::thread;
use std::time::Duration;

// =============================================================================
// BATCH SCHEDULING EDGE CASES
// =============================================================================

#[test]
fn test_scheduler_with_max_tokens_builder() {
    let scheduler = Scheduler::new(32, 1000).with_max_tokens(4096);
    // Verify the builder pattern works and scheduler is usable
    assert_eq!(scheduler.waiting_count(), 0);
}

#[test]
fn test_scheduler_multiple_requests_fifo_within_same_priority() {
    let mut scheduler = Scheduler::new(1, 100);
    let mut kv_cache = PagedKvCache::new(100, 16, 8, 64);

    // Add multiple requests with same priority
    let id1 = scheduler
        .add_request_with_priority(vec![1], 10, Priority::Normal)
        .expect("first request");
    // Small delay to ensure arrival time differs
    let id2 = scheduler
        .add_request_with_priority(vec![2], 10, Priority::Normal)
        .expect("second request");

    // Only one slot available - should pick first (FIFO within same priority)
    let output = scheduler.schedule(&mut kv_cache, 0).expect("schedule");
    assert_eq!(output.scheduled_request_ids.len(), 1);
    assert_eq!(output.scheduled_request_ids[0], id1);

    // Verify id2 is still waiting
    let req2 = scheduler.get_request(id2).expect("get request");
    assert_eq!(req2.state, SequenceState::Waiting);
}

#[test]
fn test_scheduler_empty_input_ids() {
    let mut scheduler = Scheduler::new(32, 1000);
    let mut kv_cache = PagedKvCache::new(100, 16, 8, 64);

    // Empty input should still work
    let request_id = scheduler.add_request(vec![], 10).expect("empty input");
    let output = scheduler.schedule(&mut kv_cache, 0).expect("schedule");

    assert_eq!(output.scheduled_request_ids.len(), 1);
    assert_eq!(output.scheduled_request_ids[0], request_id);
}

#[test]
fn test_scheduler_zero_max_tokens() {
    let mut scheduler = Scheduler::new(32, 1000);

    // Zero max tokens - should be immediately complete
    let request_id = scheduler.add_request(vec![1, 2, 3], 0).expect("zero max tokens");
    let request = scheduler.get_request(request_id).expect("get request");

    // With 0 max_tokens and no generated tokens, remaining_tokens = 0
    assert_eq!(request.remaining_tokens(), 0);
}

#[test]
fn test_scheduler_request_wait_time_tracking() {
    let mut scheduler = Scheduler::new(32, 1000);

    let request_id = scheduler.add_request(vec![1], 10).expect("add request");

    // Small delay
    thread::sleep(Duration::from_millis(5));

    let request = scheduler.get_request(request_id).expect("get request");
    // Wait time should be non-zero
    assert!(request.wait_time().as_nanos() > 0);
}

#[test]
fn test_scheduler_output_default() {
    let output = SchedulerOutput::default();
    assert!(output.is_empty());
    assert_eq!(output.total_tokens(), 0);
    assert!(output.scheduled_seq_ids.is_empty());
    assert!(output.preempted_seq_ids.is_empty());
    assert!(output.completed_request_ids.is_empty());
}

#[test]
fn test_scheduler_stats_default() {
    let stats = SchedulerStats::default();
    assert_eq!(stats.total_requests, 0);
    assert_eq!(stats.completed_requests, 0);
    assert_eq!(stats.preemptions, 0);
    assert_eq!(stats.avg_wait_time_ms, 0.0);
    assert_eq!(stats.avg_ttft_ms, 0.0);
    assert_eq!(stats.queue_depth, 0);
    assert_eq!(stats.running_count, 0);
}

// =============================================================================
// REQUEST PRIORITIZATION EDGE CASES
// =============================================================================

#[test]
fn test_priority_all_levels() {
    // Verify all priority levels can be compared
    assert!(Priority::Low < Priority::Normal);
    assert!(Priority::Normal < Priority::High);
    assert!(Priority::High < Priority::Critical);

    // Test equality
    assert_eq!(Priority::Low, Priority::Low);
    assert_eq!(Priority::Critical, Priority::Critical);
}

#[test]
fn test_scheduler_preemption_critical_over_low() {
    let mut scheduler = Scheduler::new(1, 100);
    let mut kv_cache = PagedKvCache::new(100, 16, 8, 64);

    // Add low priority request first and schedule it
    let low_id = scheduler
        .add_request_with_priority(vec![1], 10, Priority::Low)
        .expect("low priority");
    let _ = scheduler.schedule(&mut kv_cache, 0).expect("first schedule");

    // Verify low priority is running
    assert_eq!(scheduler.running_count(), 1);

    // Add critical priority request
    let _critical_id = scheduler
        .add_request_with_priority(vec![2], 10, Priority::Critical)
        .expect("critical priority");

    // Schedule again - critical should preempt low
    let output = scheduler.schedule(&mut kv_cache, 0).expect("second schedule");

    // Low priority should be preempted
    let low_request = scheduler.get_request(low_id).expect("get low request");
    assert!(
        low_request.state == SequenceState::Preempted
            || !output.preempted_seq_ids.is_empty()
            || scheduler.preempted_count() > 0
    );
}

#[test]
fn test_scheduler_no_preemption_same_priority() {
    let mut scheduler = Scheduler::new(1, 100);
    let mut kv_cache = PagedKvCache::new(100, 16, 8, 64);

    // Add and schedule first normal priority
    let _first_id = scheduler
        .add_request_with_priority(vec![1], 10, Priority::Normal)
        .expect("first");
    let _ = scheduler.schedule(&mut kv_cache, 0).expect("first schedule");

    // Add second normal priority
    let _second_id = scheduler
        .add_request_with_priority(vec![2], 10, Priority::Normal)
        .expect("second");

    // Schedule - no preemption should occur for same priority
    let output = scheduler.schedule(&mut kv_cache, 0).expect("second schedule");
    assert_eq!(output.preempted_seq_ids.len(), 0);
}

// =============================================================================
// QUEUE MANAGEMENT EDGE CASES
// =============================================================================

#[test]
fn test_scheduler_queue_full_error_message() {
    let mut scheduler = Scheduler::new(32, 2);

    let _ = scheduler.add_request(vec![1], 10).expect("first");
    let _ = scheduler.add_request(vec![2], 10).expect("second");

    let result = scheduler.add_request(vec![3], 10);
    match result {
        Err(SchedulerError::QueueFull { capacity }) => {
            assert_eq!(capacity, 2);
        }
        _ => panic!("Expected QueueFull error"),
    }
}

#[test]
fn test_scheduler_error_display() {
    let queue_full = SchedulerError::QueueFull { capacity: 100 };
    let display = format!("{queue_full}");
    assert!(display.contains("100"));
    assert!(display.contains("full"));

    let not_found = SchedulerError::RequestNotFound(42);
    let display = format!("{not_found}");
    assert!(display.contains("42"));
    assert!(display.contains("not found"));

    let invalid_state = SchedulerError::InvalidState("test error".to_string());
    let display = format!("{invalid_state}");
    assert!(display.contains("test error"));
}

#[test]
fn test_scheduler_request_not_found() {
    let scheduler = Scheduler::new(32, 1000);
    assert!(scheduler.get_request(999).is_none());
}

#[test]
fn test_scheduler_preempted_count() {
    let mut scheduler = Scheduler::new(1, 100);
    let mut kv_cache = PagedKvCache::new(100, 16, 8, 64);

    // Setup: schedule low priority
    let _ = scheduler
        .add_request_with_priority(vec![1], 10, Priority::Low)
        .expect("low");
    let _ = scheduler.schedule(&mut kv_cache, 0).expect("schedule");

    // Add high priority to trigger preemption
    let _ = scheduler
        .add_request_with_priority(vec![2], 10, Priority::Critical)
        .expect("high");
    let _ = scheduler.schedule(&mut kv_cache, 0).expect("schedule2");

    // Check preempted count - verify scheduler tracks it correctly
    let preempted = scheduler.preempted_count();
    // Just verify we can call the method without panicking
    let _ = preempted;
}

// =============================================================================
// ERROR HANDLING PATHS
// =============================================================================

#[test]
fn test_complete_request_nonexistent() {
    let mut scheduler = Scheduler::new(32, 1000);
    let mut kv_cache = PagedKvCache::new(100, 16, 8, 64);

    // Complete a non-existent request - should not panic
    scheduler.complete_request(999, &mut kv_cache);

    // Stats should not change
    assert_eq!(scheduler.stats().completed_requests, 0);
}

#[test]
fn test_update_after_iteration_nonexistent_request() {
    let mut scheduler = Scheduler::new(32, 1000);

    let mut generated = HashMap::new();
    generated.insert(999, 42u32);

    // Should not panic when updating non-existent request
    scheduler.update_after_iteration(&generated);
}

#[test]
fn test_scheduler_request_is_complete_edge_cases() {
    let mut request = SchedulerRequest::new(0, vec![1, 2, 3], 5);

    // Not complete with empty generated tokens
    assert!(!request.is_complete(0));

    // Complete when last token is EOS
    request.generated_tokens = vec![10, 0]; // 0 is EOS
    assert!(request.is_complete(0));

    // Complete when max_tokens reached
    let mut request2 = SchedulerRequest::new(1, vec![1], 2);
    request2.generated_tokens = vec![10, 20];
    assert!(request2.is_complete(999)); // 999 is not EOS but max reached
}

// =============================================================================
// SLOT MANAGER EDGE CASES
// =============================================================================

#[test]
fn test_slot_state_default_is_idle() {
    let state = SlotState::default();
    assert_eq!(state, SlotState::Idle);
}

#[test]
fn test_slot_tokens_per_second_zero_time() {
    let slot = Slot::new(0);
    // No generation, so tokens_per_second should be 0
    assert_eq!(slot.tokens_per_second(), 0.0);
}

#[test]
fn test_slot_finish_without_generation_start() {
    let mut slot = Slot::new(0);
    slot.assign(1, vec![1, 2, 3], 10);
    // Finish without starting generation
    slot.finish();

    assert!(slot.is_idle());
    assert_eq!(slot.generation_time_ms, 0.0);
}

#[test]
fn test_slot_manager_empty_utilization() {
    let manager = SlotManager::new(0, 2048);
    assert_eq!(manager.utilization(), 0.0);
}

#[test]
fn test_slot_manager_generating_slots() {
    let mut manager = SlotManager::new(4, 2048);

    manager.assign_request(vec![1], 10);
    manager.assign_request(vec![2], 10);

    // Start generating on slot 0 only
    manager.get_slot_mut(0).expect("slot 0").start_generation(1.0);

    let generating: Vec<_> = manager.generating_slots().collect();
    assert_eq!(generating.len(), 1);
}

#[test]
fn test_slot_manager_aggregate_tokens_per_second() {
    let manager = SlotManager::new(4, 2048);
    // No slots generating, so aggregate should be 0
    assert_eq!(manager.aggregate_tokens_per_second(), 0.0);
}

// =============================================================================
// MICRO BATCH EDGE CASES
// =============================================================================

#[test]
fn test_micro_batch_with_capacity() {
    let batch = MicroBatch::with_capacity(100);
    assert!(batch.is_empty());
    assert_eq!(batch.len(), 0);
}

#[test]
fn test_micro_batch_decode_only() {
    let mut batch = MicroBatch::new();

    // Add only decode tokens (not prompt)
    batch.add_token(BatchToken::new(1, 0, 5, false));
    batch.add_token(BatchToken::new(2, 1, 10, false));

    assert!(batch.is_decode());
    assert!(!batch.is_prefill());
    assert!(!batch.is_mixed());
    assert_eq!(batch.n_decode_tokens, 2);
    assert_eq!(batch.n_prompt_tokens, 0);
}

#[test]
fn test_micro_batch_multiple_sequences() {
    let mut batch = MicroBatch::new();

    // Tokens from different sequences
    batch.add_token(BatchToken::new(1, 0, 0, true));
    batch.add_token(BatchToken::new(2, 1, 0, true));
    batch.add_token(BatchToken::new(3, 2, 0, true));

    assert_eq!(batch.num_sequences(), 3);
}

#[test]
fn test_batch_type_default() {
    let batch_type = BatchType::default();
    assert_eq!(batch_type, BatchType::Decode);
}

// =============================================================================
// SEQUENCE BATCH EDGE CASES
// =============================================================================

#[test]
fn test_sequence_batch_get_nonexistent() {
    let batch = SequenceBatch::new(8);
    assert!(batch.get(999).is_none());
}

#[test]
fn test_sequence_batch_get_mut_nonexistent() {
    let mut batch = SequenceBatch::new(8);
    assert!(batch.get_mut(999).is_none());
}

#[test]
fn test_sequence_batch_remove_nonexistent() {
    let mut batch = SequenceBatch::new(8);
    batch.add_sequence(SequenceBatchEntry::new(0, 0, 1));
    assert!(batch.remove_sequence(999).is_none());
}

#[test]
fn test_sequence_batch_utilization_zero_max() {
    // Edge case: max_batch_size is 0
    let batch = SequenceBatch {
        sequences: Vec::new(),
        max_batch_size: 0,
        utilization: 0.0,
    };
    // Should not panic
    assert_eq!(batch.utilization, 0.0);
}

// =============================================================================
// BATCH SCHEDULER EDGE CASES
// =============================================================================

#[test]
fn test_batch_scheduler_start_decode_nonexistent() {
    let mut scheduler = BatchScheduler::new();
    assert!(!scheduler.start_decode(999, 0));
}

#[test]
fn test_batch_scheduler_config_accessor() {
    let config = BatchConfig::default()
        .with_max_tokens(256)
        .with_max_sequences(4);
    let scheduler = BatchScheduler::with_config(config);

    assert_eq!(scheduler.config().max_ubatch_tokens, 256);
    assert_eq!(scheduler.config().max_sbatch_sequences, 4);
}

#[test]
fn test_batch_scheduler_create_empty_ubatch() {
    let mut scheduler = BatchScheduler::new();
    let ubatch = scheduler.create_ubatch();
    assert!(ubatch.is_empty());
}

#[test]
fn test_batch_scheduler_mixed_prefill_decode() {
    let config = BatchConfig::default()
        .with_max_tokens(100)
        .with_max_sequences(4);
    let mut scheduler = BatchScheduler::with_config(BatchConfig {
        prefer_pure_decode: false,
        ..config
    });

    // Add prefill sequence
    scheduler.add_sequence(0, 1, vec![10, 20, 30]);

    // Add decode sequence (start decode on another)
    let seq2 = scheduler.add_sequence(1, 2, vec![40, 50]).expect("seq2");
    scheduler.start_decode(seq2, 2);

    let ubatch = scheduler.create_ubatch();
    // Should be mixed since prefer_pure_decode is false
    assert!(!ubatch.is_empty());
}

#[test]
fn test_batch_stats_default() {
    let stats = BatchStats::default();
    assert_eq!(stats.ubatches_created, 0);
    assert_eq!(stats.sbatches_created, 0);
    assert_eq!(stats.tokens_processed, 0);
    assert_eq!(stats.prefill_tokens, 0);
    assert_eq!(stats.decode_tokens, 0);
    assert_eq!(stats.avg_ubatch_size, 0.0);
    assert_eq!(stats.avg_sbatch_size, 0.0);
}

// =============================================================================
// DYNAMIC PRIORITY SCHEDULER EDGE CASES
// =============================================================================

#[test]
fn test_dynamic_request_is_expired_no_deadline() {
    let request = DynamicRequest::new(0, vec![1], 10);
    assert!(!request.is_expired());
}

#[test]
fn test_dynamic_request_is_urgent_no_deadline() {
    let request = DynamicRequest::new(0, vec![1], 10);
    assert!(!request.is_urgent());
    assert_eq!(request.urgency_score(), 0.0);
}

#[test]
fn test_dynamic_request_urgency_zero_target() {
    let request = DynamicRequest::new(0, vec![1], 10).with_deadline(Deadline {
        target_latency_ms: 0,
        hard_deadline_ms: None,
        sla_target: 0.99,
    });
    assert_eq!(request.urgency_score(), 0.0);
}

#[test]
fn test_dynamic_scheduler_schedule_no_slots() {
    let mut scheduler = DynamicPriorityScheduler::new(1024);
    scheduler.add_simple_request(vec![1], 10);

    // Schedule with 0 slots
    let batch = scheduler.schedule(0);
    assert!(batch.is_empty());
}

#[test]
fn test_dynamic_scheduler_complete_nonexistent() {
    let mut scheduler = DynamicPriorityScheduler::new(1024);
    let completed = scheduler.complete_request(999);
    assert!(completed.is_none());
}

#[test]
fn test_dynamic_scheduler_sla_compliance_no_requests() {
    let scheduler = DynamicPriorityScheduler::new(1024);
    // No requests completed, compliance should be 1.0 (100%)
    assert_eq!(scheduler.sla_compliance_rate(), 1.0);
}

#[test]
fn test_dynamic_priority_config_all_options() {
    let config = DynamicPriorityConfig::with_budgets([0.25, 0.25, 0.25, 0.25])
        .no_promotion()
        .with_promotion_interval(10000);

    assert!(!config.enable_age_promotion);
    assert_eq!(config.promotion_interval_ms, 10000);
    assert_eq!(config.priority_budgets[0], 0.25);
}

#[test]
fn test_dynamic_scheduler_stats_default() {
    let stats = DynamicSchedulerStats::default();
    assert_eq!(stats.total_requests, 0);
    assert_eq!(stats.completed_requests, 0);
    assert_eq!(stats.sla_met, 0);
    assert_eq!(stats.sla_missed, 0);
    assert_eq!(stats.dropped_requests, 0);
    assert_eq!(stats.promotions, 0);
    assert_eq!(stats.avg_ttft_ms, 0.0);
    assert_eq!(stats.p99_ttft_ms, 0.0);
}

#[test]
fn test_dynamic_scheduler_no_fair_share() {
    let config = DynamicPriorityConfig {
        enable_fair_share: false,
        ..Default::default()
    };
    let mut scheduler = DynamicPriorityScheduler::with_config(1024, config);

    scheduler.add_request(vec![1], 50, Priority::Low, None);
    scheduler.add_request(vec![2], 50, Priority::High, None);

    // Without fair share, budget is not divided by priority
    let batch = scheduler.schedule(2);
    assert_eq!(batch.len(), 2);
}

// =============================================================================
// CHUNKED PREFILL EDGE CASES
// =============================================================================

#[test]
fn test_chunked_prefill_state_zero_chunk_size() {
    // Edge case: if chunk_size somehow becomes 0, total_chunks would be infinity
    // But div_ceil handles this - let's test with valid values
    let state = ChunkedPrefillState::new(0, 100, 1);
    assert_eq!(state.total_chunks, 100);
}

#[test]
fn test_chunked_prefill_state_exact_chunk_fit() {
    // Tokens exactly fit into chunks
    let state = ChunkedPrefillState::new(0, 1024, 512);
    assert_eq!(state.total_chunks, 2);
}

#[test]
fn test_chunked_prefill_scheduler_get_state_nonexistent() {
    let scheduler = ChunkedPrefillScheduler::default();
    assert!(scheduler.get_state(999).is_none());
}

#[test]
fn test_chunked_prefill_scheduler_next_chunk_empty() {
    let mut scheduler = ChunkedPrefillScheduler::default();
    assert!(scheduler.next_chunk().is_none());
}

#[test]
fn test_chunked_prefill_scheduler_complete_chunk_nonexistent() {
    let mut scheduler = ChunkedPrefillScheduler::default();
    // Should not panic
    scheduler.complete_chunk(999, 100, 10);
    assert_eq!(scheduler.stats().chunks_processed, 0);
}

#[test]
fn test_chunked_prefill_stats_chunking_rate_zero() {
    let stats = ChunkedPrefillStats::default();
    // Both chunked and bypassed are 0
    assert_eq!(stats.chunking_rate(), 0.0);
}

#[test]
fn test_chunked_prefill_config_builder() {
    let config = ChunkedPrefillConfig::default().with_chunk_size(128);
    assert_eq!(config.chunk_size, 128);
    assert!(config.enabled);
}

#[test]
fn test_chunked_prefill_scheduler_round_robin() {
    // Test with boost_partial_prefill = false for round-robin behavior
    let config = ChunkedPrefillConfig {
        boost_partial_prefill: false,
        min_prompt_length: 10,
        chunk_size: 50,
        ..Default::default()
    };
    let mut scheduler = ChunkedPrefillScheduler::new(config);

    // Submit long sequence
    scheduler.submit(100);
    scheduler.complete_chunk(0, 50, 10);

    // After completion, should move to back of queue
    assert_eq!(scheduler.queue_len(), 1);
}

// =============================================================================
// SERIALIZATION TESTS
// =============================================================================

#[test]
fn test_scheduler_stats_serialization_roundtrip() {
    let stats = SchedulerStats {
        total_requests: 100,
        completed_requests: 90,
        preemptions: 5,
        avg_wait_time_ms: 15.5,
        avg_ttft_ms: 50.0,
        queue_depth: 10,
        running_count: 8,
    };

    let json = serde_json::to_string(&stats).expect("serialize");
    let parsed: SchedulerStats = serde_json::from_str(&json).expect("deserialize");

    assert_eq!(parsed.total_requests, 100);
    assert_eq!(parsed.completed_requests, 90);
    assert_eq!(parsed.preemptions, 5);
    assert!((parsed.avg_wait_time_ms - 15.5).abs() < 0.01);
}

#[test]
fn test_dynamic_scheduler_stats_serialization_roundtrip() {
    let stats = DynamicSchedulerStats {
        total_requests: 50,
        completed_requests: 45,
        sla_met: 40,
        sla_missed: 5,
        dropped_requests: 5,
        promotions: 10,
        avg_ttft_ms: 100.0,
        p99_ttft_ms: 500.0,
        tokens_by_priority: [10, 100, 200, 50],
        queue_depth_by_priority: [1, 5, 3, 0],
    };

    let json = serde_json::to_string(&stats).expect("serialize");
    let parsed: DynamicSchedulerStats = serde_json::from_str(&json).expect("deserialize");

    assert_eq!(parsed.total_requests, 50);
    assert_eq!(parsed.sla_met, 40);
    assert_eq!(parsed.tokens_by_priority[2], 200);
}

#[test]
fn test_chunked_prefill_stats_serialization_roundtrip() {
    let stats = ChunkedPrefillStats {
        chunked_sequences: 10,
        bypassed_sequences: 5,
        chunks_processed: 20,
        decode_interleaves: 8,
        total_chunk_latency_ms: 500,
        max_chunk_latency_ms: 100,
        prefix_cache_hits: 3,
    };

    let json = serde_json::to_string(&stats).expect("serialize");
    let parsed: ChunkedPrefillStats = serde_json::from_str(&json).expect("deserialize");

    assert_eq!(parsed.chunked_sequences, 10);
    assert_eq!(parsed.chunks_processed, 20);
}

#[test]
fn test_batch_config_serialization() {
    let config = BatchConfig {
        max_ubatch_tokens: 256,
        max_sbatch_sequences: 4,
        prefer_pure_decode: false,
        max_context_length: 4096,
        dynamic_batching: false,
    };

    let json = serde_json::to_string(&config).expect("serialize");
    let parsed: BatchConfig = serde_json::from_str(&json).expect("deserialize");

    assert_eq!(parsed.max_ubatch_tokens, 256);
    assert_eq!(parsed.max_sbatch_sequences, 4);
    assert!(!parsed.prefer_pure_decode);
}

// =============================================================================
// INTEGRATION-STYLE TESTS
// =============================================================================

#[test]
fn test_full_scheduler_lifecycle() {
    let mut scheduler = Scheduler::new(2, 100);
    let mut kv_cache = PagedKvCache::new(100, 16, 8, 64);

    // Add multiple requests
    let id1 = scheduler.add_request(vec![1, 2, 3], 5).expect("req1");
    let id2 = scheduler.add_request(vec![4, 5], 5).expect("req2");
    let id3 = scheduler.add_request(vec![6], 5).expect("req3");

    // Schedule first batch
    let output = scheduler.schedule(&mut kv_cache, 0).expect("schedule1");
    assert_eq!(output.scheduled_request_ids.len(), 2);
    assert_eq!(scheduler.running_count(), 2);
    assert_eq!(scheduler.waiting_count(), 1);

    // Update with generated tokens
    let mut generated = HashMap::new();
    generated.insert(id1, 100);
    generated.insert(id2, 101);
    scheduler.update_after_iteration(&generated);

    // Complete first request
    scheduler.complete_request(id1, &mut kv_cache);
    assert_eq!(scheduler.running_count(), 1);
    assert_eq!(scheduler.stats().completed_requests, 1);

    // Schedule again - should pick up id3
    let output2 = scheduler.schedule(&mut kv_cache, 0).expect("schedule2");
    assert!(output2.scheduled_request_ids.contains(&id3));
}

#[test]
fn test_batch_scheduler_full_lifecycle() {
    let mut scheduler = BatchScheduler::new();

    // Add sequences
    let seq1 = scheduler.add_sequence(0, 100, vec![1, 2, 3]).expect("seq1");
    let seq2 = scheduler.add_sequence(1, 101, vec![4, 5]).expect("seq2");

    // Create prefill ubatch
    let ubatch1 = scheduler.create_ubatch();
    assert!(ubatch1.is_prefill());
    assert_eq!(ubatch1.len(), 5); // 3 + 2 tokens

    // Transition to decode
    scheduler.start_decode(seq1, 3);
    scheduler.start_decode(seq2, 2);

    // Create decode ubatch
    let ubatch2 = scheduler.create_ubatch();
    assert!(ubatch2.is_decode());
    assert_eq!(ubatch2.len(), 2); // 1 per sequence

    // Complete sequences
    let completed1 = scheduler.complete_sequence(seq1);
    assert!(completed1.is_some());

    let completed2 = scheduler.complete_sequence(seq2);
    assert!(completed2.is_some());

    assert_eq!(scheduler.num_sequences(), 0);
}

#[test]
fn test_dynamic_scheduler_with_deadline_lifecycle() {
    let mut scheduler = DynamicPriorityScheduler::new(1024);

    // Add request with long deadline (will be met)
    let id = scheduler.add_request(
        vec![1, 2, 3],
        10,
        Priority::High,
        Some(Deadline::with_target(100_000)), // 100 seconds
    );

    // Schedule
    let batch = scheduler.schedule(1);
    assert_eq!(batch.len(), 1);
    assert_eq!(batch[0].0, id);

    // Complete
    let completed = scheduler.complete_request(id);
    assert!(completed.is_some());
    assert_eq!(scheduler.stats().sla_met, 1);
}

#[test]
fn test_chunked_prefill_full_lifecycle() {
    let config = ChunkedPrefillConfig {
        chunk_size: 100,
        min_prompt_length: 50,
        ..Default::default()
    };
    let mut scheduler = ChunkedPrefillScheduler::new(config);

    // Submit sequence that needs chunking
    let (seq_id, uses_chunking) = scheduler.submit(250);
    assert!(uses_chunking);
    assert_eq!(scheduler.queue_len(), 1);

    // Process all chunks
    let mut chunks_processed = 0;
    while let Some((id, range)) = scheduler.next_chunk() {
        assert_eq!(id, seq_id);
        let tokens = range.end - range.start;
        scheduler.complete_chunk(id, tokens, 10);
        chunks_processed += 1;
    }

    assert_eq!(chunks_processed, 3); // ceil(250/100) = 3
    assert!(!scheduler.has_pending_prefill(seq_id));
}
